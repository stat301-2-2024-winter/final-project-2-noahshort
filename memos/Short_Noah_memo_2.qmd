---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Noah Short"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[My Final Project Repo](https://github.com/stat301-2-2024-winter/final-project-2-noahshort.git)

:::

## Assessment Metric
My assessment metric will be RMSE. The predictor variable, a player's WAR in the following season or next_year_war_total, has a small range of only about 34 units. This means that the rmse will be simple to interpret in the grand scheme of overall accuracy, especially since most of the observations are clustered towards the center of the distribution.

When thinking about the limitations of RMSE, it is natural to think about certain predictors that are omitted and that the model CANNOT directly explain, or in other words, what predictors may be contributing to the error that we can't directly test for. In my context, the largest ones include, but are not limited to: injury, seasonal intricacies (ex: pandemic), evolution of basketball strategy over time, and years of experience in the league.

## Analysis Plan
Some context for the tidied and wrangled data set is important to consider here. The data set has 11,583 observations, each of which represents an NBA player's metrics for that season and includes some biographical information for the player. A missingness report is pasted below:

```{r}
#| label: Missingness report
#| echo: false
#| warning: false

setwd("/Users/noahshort/Desktop/stat-301-2/final-project/final-project-2")
library(here)
load("results/missing_report_predictor.rda")
missing_report_predictor
```

We see here that the data set is still very clean. The two variables with missingness can be easily explained.

`next_season_war_total` is missing for every player's final season. This means that a missing value indicates that a player retired, which could be due to age or injury, or that a player did not perform well enough to play in the NBA the following year. I want to keep these values missing because there are many players who stay in the league with a negative WAR, so turning these values into zeroes would not be perfectly representative. I will address imputation when crafting my recipes

`draft_number` is missing for every player who did not enter the league through the formal NBA draft. I don't anticipate using this variable in recipes, and I accommodate for this missingness in the `draft_round` variable, which is a factor with 3 levels: 1, 2, and U for un-drafted.

The initial split of the data will be done with respect to season with the most recent seasons being part of the testing set and all others being part of the training set. This is done because the natural future application of this model is with the future in mind, so the model should be tested in that same manner, based on the data we have at our disposal. The below table will inspire the exact proportion of the split:

```{r}
#| label: Table for Split
#| echo: false
#| warning: false

setwd("/Users/noahshort/Desktop/stat-301-2/final-project/final-project-2")
library(tidyverse)

complete <- read_csv("data/complete_data_fp2.csv")

complete |> 
  group_by(season) |> 
  summarize(n = n()) |> knitr::kable()
complete |> 
  summarize(n = n()) |> knitr::kable()
```
If we use seasons 21, 20, 19, and 18 (ending in 2021 back to 2018, respectively) as the testing set, the proportion is: 

(540 + 526 + 526 + 537) / 11583 = .18

So 82% of the data (about 9450 observations) is in the training set and 18% (about 2130 obs.) is in the testing set. This will be the initial split.

I will then use v-fold cross-validation with 6 folds and 5 repeats to resample the training set, with stratification based on the outcome variable, `next_season_war_total`. I want to use this stratification method with v-fold cross validation across the data from 1997 - 2017 because I am looking for aggregate results across all of these seasons and all player output levels. 6 folds and 5 repeats are relatively arbitrary choices, but I want to make sure each fold is large enough to limit the influence of seasonal intricacies. My training set is large enough such that I feel 30 trainings per model is a good number.


My 6 models will be the below types:

- Null Model, no tuning

- Linear Model, no tuning

- Elastic Net, tuning on penalty and mixture

- K-Nearest Neighbors, tuning on neighbors

- Random Forest, tuning on mtry and min_n

- Boosted Trees, tuning on mtry, min_n, and learn_rate


I plan to use recipes: 1 for the baseline model, 2 for linear models, and 2 for tree-based models. The first recipe for both linear and tree-based models will be based heavily on statistics in my data, while the second will incorporate more biographical information, such as draft round or home country.


## Model Progress

I created a very basic recipe that uses 7 of my statistical predictors in estimating the outcome variable. It is saved as `basketball_recipe_memo.rda` in the reicpes folder.

I fit a null model and basic lm model to this recipe, the results are in the results folder and discussed in the next section.


## Model Fit Results

```{r}
#| label: Model Fit Results
#| echo: false
#| warning: false

library(here)

setwd("/Users/noahshort/Desktop/stat-301-2/final-project/final-project-2")
load("results/null_rmse_memo.rda")
load("results/lm_rmse_memo.rda")

null_rmse_memo |> knitr::kable()
lm_rmse_memo |> knitr::kable()
```

Promising results here. The null model stinks, as missing by 3 wins per player is abysmal, but improvements are large in the linear model. Hopefully we can get even better with more complex models.

## Summary and Next Steps

So far I have all the data ready to work with and a baseline recipe and model crafted. The next steps are as follows:

1. Dig into the predictor variables to determime transformations that may be valuable to the recipes

2. Consider which method of imputation is best for the outcome variable, and consider carrying it out in the pre-processing steps.

3. Build recipes 

4. Build models

5. Analyze, analyze, analyze.


## Data Sources
[Advanced Metrics Raptor Data - FiveThirtyEight](https://github.com/fivethirtyeight/data/blob/master/nba-raptor/historical_RAPTOR_by_player.csv)

[Draft Data - Kaggle](https://www.kaggle.com/datasets/justinas/nba-players-data?resource=download)

