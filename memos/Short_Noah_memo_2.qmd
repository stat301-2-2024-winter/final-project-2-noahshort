---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Noah Short"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[My Final Project Repo](https://github.com/stat301-2-2024-winter/final-project-2-noahshort.git)

:::

## Assessment Metric
My assessment metric will be R-squared. The predictor variable, a player's WAR in the following season or next_year_war_total, has a small range of only about 34 units. This means that a strong model will have very small residual values that may be hard to interpret. Furthermore, I personally think R-squared is a strong measure for models in general beyond this project alone, in that the R-squared value can be understood within a general context of "how good is this model" on its own, where the intricacies of the model need not be understood.

When thinking about the limitations of R-squared, it is natural to think about certain predictors that are omitted and that the model CANNOT directly explain. In my context, the largest ones include, but are not limited to: injury, seasonal intricacies (ex: pandemic), evolution of basketball strategy over time, and years of experience in the league.

## Analysis Plan
Some context for the tidied and wrangled data set is important to consider here. The data set has 11,583 observations, each of which represents an NBA player's metrics for that season and includes some biographical information for the player. A missingness report is pasted below:

```{r}
#| label: Missingness report
#| echo: false
#| warning: false

setwd("/Users/noahshort/Desktop/stat-301-2/final-project/final-project-2")
library(here)
load("results/missing_report_predictor.rda")
missing_report_predictor
```

We see here that the data set is still very clean. The two variables with missingness can be easily explained.

`next_season_war_total` is missing for every player's final season. This means that a missing value indicates that a player retired, which could be due to age or injury, or that a player did not perform well enough to play in the NBA the following year. I want to keep these values missing because there are many players who stay in the league with a negative WAR, so turning these values into zeroes would not be perfectly representative.I will address imputation when crafting my recipes

`draft_number` is missing for every player who did not enter the league through the formal NBA draft. I don't anticipate using this variable in recipes, and I accommodate for this missingness in the `draft_round` variable, which is a factor with 3 levels: 1, 2, and U for un-drafted.

The initial split of the data will be done with respect to season with the most recent seasons being part of the testing set and all others being part of the training set. This is done because the natural future application of this model is with the future in mind, so the model should be tested in that same manner, based on the data we have at our disposal. The below table will inspire the exact proportion of the split:

```{r}
#| label: Table for Split
#| echo: false
#| warning: false

setwd("/Users/noahshort/Desktop/stat-301-2/final-project/final-project-2")
library(tidyverse)

complete <- read_csv("data/complete_data_fp2.csv")

complete |> 
  group_by(season) |> 
  summarize(n = n()) |> knitr::kable()
complete |> 
  summarize(n = n()) |> knitr::kable()
```
If we use seasons 21, 20, 19, and 18 (ending in 2021 back to 2018, respectively) as the testing set, the proportion is: 

(540 + 526 + 526 + 537) / 11583 = .18

So 82% of the data (about 9450 observations) is in the training set and 18% (about 2130 obs.) is in the testing set. This will be the initial split.

I will then use v-fold cross-validation with 10 folds and 4 repeats to resample the training set, with stratification based on the outcome variable, `next_season_war_total`. I want to use this stratification method with v-fold cross validation across the data from 1997 - 2017 because I am looking for aggregate results across all of these seasons and all player output levels. 10 folds and 4 repeats are relatively arbitrary choices, but the book recommends 10 folds and our training set is pretty large, so 4 repeats will be sufficient. In total, there will be 40 fits per model.


My 6 models will be the below types:

- Null Model, no tuning

- Linear Model, no tuning

- Elastic Net, tuning on penalty and mixture

- K-Nearest Neighbors, tuning on neighbors

- Random Forest, tuning on mtry and min_n

- Boosted Trees, tuning on mtry, min_n, and learn_rate


I plan to use recipes: 1 for the baseline model, 2 for linear models, and 2 for tree-based models. The first recipe for both linear and tree-based models will be based heavily on statistics in my data, while the second will incorporate more biographical information, such as draft round or home country.


## Model Progress


## Model Fit Results


## Summary


## Data Sources
[Advanced Metrics Raptor Data - FiveThirtyEight](https://github.com/fivethirtyeight/data/blob/master/nba-raptor/historical_RAPTOR_by_player.csv)

[Draft Data - Kaggle](https://www.kaggle.com/datasets/justinas/nba-players-data?resource=download)

